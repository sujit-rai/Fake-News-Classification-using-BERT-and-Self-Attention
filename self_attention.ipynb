{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"self_attention.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"VvLZNYeBairD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ca23eac8-9113-4ef7-8cdf-3f7ab9add7ba","executionInfo":{"status":"ok","timestamp":1565008457653,"user_tz":-330,"elapsed":23180,"user":{"displayName":"Sujit Rai","photoUrl":"","userId":"07331316107733932435"}}},"source":["import numpy as np\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SGmhWFDdapVD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":972},"outputId":"3c973f72-30ad-4676-f5f4-eeebe9f5e721","executionInfo":{"status":"ok","timestamp":1565009098197,"user_tz":-330,"elapsed":663714,"user":{"displayName":"Sujit Rai","photoUrl":"","userId":"07331316107733932435"}}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","n_labels = 6\n","\n","GDFS_path = \"/content/drive/My Drive/Colab Notebooks/fake_lstm/\"\n","\n","def sample(arr, labels, seq):\n","\tsample_sentences = []\n","\tsample_labels = []\n","\tfor i in range(len(arr)):\n","\t\ttokens = len(arr[i])\n","\t\tif tokens < seq:\n","\t\t\tinst = np.array(arr[i],dtype=np.float32)\n","\t\t\tzeros = np.zeros((seq-tokens, 60))\n","\t\t\tinst = np.concatenate([inst, zeros], axis=0)\n","\t\t\tsample_sentences.append(inst)\n","\t\t\tsample_labels.append(labels[i])\n","\tsample_sentences = np.array(sample_sentences, dtype=np.float32)\n","\tsample_labels = np.array(sample_labels, dtype=np.int32)\n","\tonehot_labels = np.eye(n_labels)[sample_labels]\n","\tprint(sample_sentences.shape)\n","\tprint(onehot_labels.shape)    \n","\treturn np.nan_to_num(sample_sentences,copy=False), onehot_labels\n","\n","\n","def load_data(type):\n","\ttrain_sentences = np.load(GDFS_path+\"train_\"+type+\"_glove_m.npy\", allow_pickle=True)\n","\ttrain_labels = np.load(GDFS_path+\"train_\"+type+\"_labels_m.npy\", allow_pickle=True)\n","\tval_sentences = np.load(GDFS_path+\"val_\"+type+\"_glove_m.npy\", allow_pickle=True)\n","\tval_labels = np.load(GDFS_path+\"val_\"+type+\"_labels_m.npy\", allow_pickle=True)\n","\ttest_sentences = np.load(GDFS_path+\"test_\"+type+\"_glove_m.npy\", allow_pickle=True)\n","\ttest_labels = np.load(GDFS_path+\"test_\"+type+\"_labels_m.npy\", allow_pickle=True)\n","\treturn train_sentences, train_labels, test_sentences, test_labels, val_sentences, val_labels\n","\n","\n","train_sentences, train_labels, test_sentences, test_labels, val_sentences, val_labels = load_data(\"binary\")\n","\n","\n","\n","\n","seq_length = 200\n","\n","epochs = 10\n","\n","batch_size = 32\n","\n","batches = train_labels.shape[0]//batch_size\n","\n","test_batches = test_labels.shape[0]//batch_size\n","\n","val_batches = val_labels.shape[0]//batch_size\n","\n","\n","train_vects, train_labels = sample(train_sentences, train_labels, seq_length)\n","test_vects, test_labels = sample(test_sentences, test_labels, seq_length)\n","val_vects, val_labels = sample(val_sentences, val_labels, seq_length)\n","\n","\n","\n","\n","x = tf.placeholder(tf.float32, [None, seq_length, 60], name=\"x\")\n","y = tf.placeholder(tf.float32, [None, n_labels], name=\"y\")\n","\n","\n","\n","\n","def self_attention(inp, nodes, name):\n","  with tf.variable_scope(name):\n","    att = tf.matmul(inp, inp, transpose_b=True)\n","    soft = tf.nn.softmax(att)\n","#     soft_s = tf.matrix_band_part(soft, soft.get_shape()[1], 0)\n","    res = tf.matmul(soft, inp)\n","    dense_in = tf.reshape(res, [batch_size*seq_length, inp.get_shape()[-1]])\n","    output = tf.nn.leaky_relu(tf.layers.dense(dense_in, nodes, name=\"dense_\"+name))\n","    output_reshaped = tf.reshape(output, [batch_size, seq_length, nodes])\n","    return output_reshaped + inp\n","\n","def dense_norm(inp, nodes, name):\n","  with tf.variable_scope(name):\n","    re = tf.reshape(inp, [batch_size*seq_length, inp.get_shape()[-1]])\n","    output = tf.nn.leaky_relu(tf.layers.dense(re, nodes, name=\"dn_\"+name))\n","    output_reshaped = tf.reshape(output, [batch_size, seq_length, nodes])\n","    return tf.contrib.layers.layer_norm(output_reshaped)\n","  \n","def self_att_norm(inp, curr, nodes, name):\n","  sa = self_attention(inp, curr, name)\n","  dn = dense_norm(sa, nodes, name)\n","  return dn\n","  \n","  \n","att1 = self_att_norm(x, 60, 60, \"att1\")\n","\n","att2 = self_att_norm(att1, 60, 32, \"att2\")\n","\n","att3 = self_att_norm(att2, 32, 16, \"att3\")\n","\n","att4 = self_att_norm(att3, 16, 8, \"att4\")\n","\n","att5 = self_att_norm(att4, 8, 8, \"att5\")\n","\n","output_final = tf.reshape(att5, [batch_size, seq_length*8])\n","\n","fc1 = tf.nn.dropout(tf.layers.dense(output_final, units=256, activation=tf.nn.leaky_relu), keep_prob=0.8)\n","\n","fc2 = tf.nn.dropout(tf.layers.dense(fc1, units=64, activation=tf.nn.leaky_relu), keep_prob=1)\n","\n","logit = tf.squeeze(tf.layers.dense(fc1, units=n_labels, activation=None))\n","\n","loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=y))\n","\n","op = tf.train.AdamOptimizer(0.0001).minimize(loss)\n","\n","pred = tf.argmax(tf.nn.sigmoid(logit), axis=1)\n","\n","actual = tf.argmax(y, axis=1)\n","\n","acc = tf.reduce_mean(tf.to_float(tf.equal(pred, actual)))\n","\n","\n","\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","  sess.run(tf.global_variables_initializer())\n","  val_accs = 0\n","  for ep in range(epochs):\n","    accs = []\n","    lss = []\n","\n","    for batch in range(batches):\n","      random_indexes = np.random.choice(train_labels.shape[0], batch_size)\n","      x_batch = train_vects[random_indexes]\n","      y_batch = train_labels[random_indexes]\n","      fd = {x: x_batch, y: y_batch}\n","      _, ls, ac = sess.run([op, loss, acc], fd)\n","      accs.append(ac)\n","      lss.append(ls)\n","    print(\"epoch : \", ep, \", loss : \", np.mean(lss), \", acc : \", np.mean(accs))\n","    accs = []\n","    lss = []\n","    for batch in range(val_batches):\n","      random_indexes = np.random.choice(val_labels.shape[0], batch_size)\n","      x_batch = val_vects[random_indexes]\n","      y_batch = val_labels[random_indexes]\n","      fd = {x: x_batch, y: y_batch}\n","      ls, ac = sess.run([loss, acc], fd)\n","      accs.append(ac)\n","      lss.append(ls)\n","    print(\"val set | loss : \", np.mean(lss), \", acc : \", np.mean(accs))\n","    if np.mean(accs) >= val_accs:\n","      saver.save(sess, \"./model_ckpt\")\n","      val_accs = np.mean(accs)\n","  accs = []\n","  saver.restore(sess, \"./model_ckpt\")\n","  print(\"saved modes val acc : \", val_accs)\n","  for batch in range(test_batches):\n","    random_indexes = np.random.choice(test_labels.shape[0], batch_size)\n","    x_batch = test_vects[random_indexes]\n","    y_batch = test_labels[random_indexes]\n","    fd = {x: x_batch, y: y_batch}\n","    ls, ac = sess.run([loss, acc], fd)\n","    accs.append(ac)\n","  print(\"test set | acc : \", np.mean(accs))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["(10143, 200, 60)\n","(10143, 6)\n","(1248, 200, 60)\n","(1248, 6)\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0805 12:34:43.785321 140267289311104 deprecation.py:323] From <ipython-input-2-60c449ea3ad7>:75: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","W0805 12:34:43.791492 140267289311104 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["(1274, 200, 60)\n","(1274, 6)\n"],"name":"stdout"},{"output_type":"stream","text":["W0805 12:34:45.416728 140267289311104 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","W0805 12:34:45.978199 140267289311104 deprecation.py:506] From <ipython-input-2-60c449ea3ad7>:104: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0805 12:34:46.038048 140267289311104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0805 12:34:47.263971 140267289311104 deprecation.py:323] From <ipython-input-2-60c449ea3ad7>:118: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["epoch :  0 , loss :  0.25640884 , acc :  0.54189456\n","val set | loss :  0.24103265 , acc :  0.559375\n","epoch :  1 , loss :  0.2309669 , acc :  0.5823242\n","val set | loss :  0.24133453 , acc :  0.5578125\n","epoch :  2 , loss :  0.22045195 , acc :  0.61552733\n","val set | loss :  0.23534496 , acc :  0.5257813\n","epoch :  3 , loss :  0.21961626 , acc :  0.6099609\n","val set | loss :  0.23099723 , acc :  0.571875\n","epoch :  4 , loss :  0.21888742 , acc :  0.60849607\n","val set | loss :  0.23073407 , acc :  0.5734375\n","epoch :  5 , loss :  0.21279731 , acc :  0.6347656\n","val set | loss :  0.22529826 , acc :  0.578125\n","epoch :  6 , loss :  0.21192713 , acc :  0.63242185\n","val set | loss :  0.23045549 , acc :  0.5570313\n","epoch :  7 , loss :  0.21352813 , acc :  0.6267578\n","val set | loss :  0.22890928 , acc :  0.5703125\n","epoch :  8 , loss :  0.20816681 , acc :  0.64394534\n","val set | loss :  0.22489782 , acc :  0.57109374\n","epoch :  9 , loss :  0.20542994 , acc :  0.64941406\n"],"name":"stdout"},{"output_type":"stream","text":["W0805 12:44:54.273107 140267289311104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stderr"},{"output_type":"stream","text":["val set | loss :  0.23419552 , acc :  0.5796875\n","saved modes val acc :  0.5796875\n","test set | acc :  0.60096157\n"],"name":"stdout"}]}]}